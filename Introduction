Introduction 

Despite advances in scale and complexity in the AI industry, there have been almost no substantive changes for years. While transformers created improvements in some applications, they are not that different from neural networks. They are an incremental improvement rather than a shift in the paradigm. Dorothy represents that paradigm shift. 

Rather than using a single model at a gigantic scale, like transformers or neural networks, Dorothy uses a modular approach that leverages multiple models at a smaller scale. Multiple data analysis models are used to analyze incoming data. Each data analysis module adds one or more data points to the data in the stream. These data points (or "tags") are analyzed by a predictive model based on previous data. The predictive model adds prediction tags to the stream, which is validated by user action. That validation is fed back into the predictive model to improve its accuracy. 

Dorothy is unique in several ways. First is its input: it is data-agnostic, meaning that it is not limited to any particular data type. It can take any form of data, as long as it can analyze that data well enough to create data points. The second is its output: it is a preference engine, meaning that it is designed to make a choice. As with the data type, it can choose anything, as long as it has enough data. The third is that it is modular. It can be expanded to the needs of the application. If more or different models are needed, they can be added or changed on the fly. Finally, it is an entirely open system. Unlike some systems, Dorothy's inner workings are visible throughout the entire process. The only black boxes here are the individual modules themselves, but all they do is provide individual data points. What Dorothy does with them can be monitored from start to finish.
